{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project')\n",
    "os.chdir('C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bonnie\\Desktop\\Jupyter\\project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 원래 책에 있던 text_loader\n",
    "# =============================================================================\n",
    "# def text_loader(self, text_file = 'data\\\\aesop\\\\data.txt' ):\n",
    "#         \n",
    "#     with open(text_file, encoding='utf-8-sig') as f:\n",
    "#         text = f.read()\n",
    "#         \n",
    "#     \n",
    "#     self.start_story = '| '  * self.seq_length\n",
    "#     \n",
    "#     # 텍스트 정제    \n",
    "#     text = text.lower()\n",
    "#     text = self.start_story + text\n",
    "#     text = text.replace('\\n\\n\\n\\n\\n', self.start_story)\n",
    "#     text = text.replace('\\n',' ') \n",
    "#     \n",
    "#     text = re.sub('  +', '. ', text).strip()\n",
    "#     text = text.replace('..', '.')\n",
    "#     \n",
    "#     text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "#     text = re.sub('\\s{2,}', ' ', text)\n",
    "#     \n",
    "#     self.text = text  \n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-239b3281f0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;31m#t1.load_network()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-239b3281f0ab>\u001b[0m in \u001b[0;36mgenerate_sequences\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 1차원짜리 y값을 원핫 인코딩으로 변환 (total_words 개수만큼)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# to_categorical 얘는 그냥 0이면 1번쨰열을 1 나머지는 0, 55면 56번째 열을 1 나머지는 0 이런식으로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bonnie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class text_LSTM:\n",
    "    def __init__(self):\n",
    "        self.seq_length = 20\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def text_loader(self, text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv' ):\n",
    "            \n",
    "        with open(text_file, encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        self.start_story = '| '  * self.seq_length\n",
    "        \n",
    "        # 텍스트 정제    \n",
    "        text = text.lower()\n",
    "        text = self.start_story + text\n",
    "        text = text.replace('\\n\\n\\n\\n\\n', self.start_story) # 줄바꿈 5번 된건 끝나고 새로 시작하는걸로 처리\n",
    "        \n",
    "        text = re.sub('[\\n]{2,}','\\n', text) #\n",
    "        \n",
    "        text = text.replace('\\n','\\n ') # 랩에 맞춰서 새로 만듬\n",
    "        \n",
    "        #text = re.sub('  +', '. ', text).strip()\n",
    "        #text = text.replace('..', '.')\n",
    "        \n",
    "        text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text) # 두번째 입력변수에 r' \\1 ' 대신 ' \\\\1 ' 써줘도 똑같음 - r 써주는 방법이 더 안전할 수 있대\n",
    "        #text = re.sub('\\s{2,}', ' ', text)\n",
    "        \n",
    "        self.text = text\n",
    "    \n",
    "    def tokenize(self):\n",
    "        # 토큰화\n",
    "        self.tokenizer = Tokenizer(char_level = False, filters = '')         # 필터에 적혀있는 애는 토큰으로 안만든대\n",
    "        self.tokenizer.fit_on_texts([self.text])                             # 여기서 리스트 하나 더 안씌우면 문자단위로 토큰화 한다.\n",
    "        self.total_words = len(self.tokenizer.word_index) + 1                # 모든 단어종류 +1 // 1 더하는 이유는, 토크나이저.word_index가 1부터 인덱싱 해서, 뒤에 to_categorical로 원핫 인코딩할때 맞춰주기 위해서임\n",
    "        self.token_list = self.tokenizer.texts_to_sequences([self.text])[0]  # text의 모든 단어들을 토큰으로 바꾼거    \n",
    "    \n",
    "    ### seq_length만큼 잘라서, 신경망에 학습시킬 수 있도록 각 seq - 다음단어 쌍을 만들어준다.\n",
    "    \n",
    "    def generate_sequences(self, step = 1):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(0, len(self.token_list) - self.seq_length, step):\n",
    "            X.append(self.token_list[i:i+self.seq_length])\n",
    "            y.append(self.token_list[i + self.seq_length])\n",
    "            \n",
    "        y = np_utils.to_categorical(y, num_classes = self.total_words) \n",
    "        # 1차원짜리 y값을 원핫 인코딩으로 변환 (total_words 개수만큼)\n",
    "        # to_categorical 얘는 그냥 0이면 1번쨰열을 1 나머지는 0, 55면 56번째 열을 1 나머지는 0 이런식으로\n",
    "        # 숫자 값 그대로 원핫 인코딩 해주는 애\n",
    "        \n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.num_seq = len(X)        \n",
    "        \n",
    "        print('시퀀스 개수:', self.num_seq, '\\n')\n",
    "    \n",
    "    def build_network(self):\n",
    "        \n",
    "        n_units = 256\n",
    "        embedding_size = 100\n",
    "        \n",
    "        # 여기서부터 신경망 시작\n",
    "        \n",
    "        text_in = Input(shape = (None,))\n",
    "        x = Embedding(self.total_words, embedding_size)(text_in)\n",
    "        x = LSTM(units = n_units)(x)    # output의 차원은 n_units 개\n",
    "        x = Dropout(rate = 0.2)(x)\n",
    "        text_out = Dense(units = self.total_words, activation = 'softmax')(x)   # 아웃풋은 단어 종류의 개수만큼 표현이 되어야 하니까\n",
    "        \n",
    "        self.model1 = Model(text_in, text_out)        \n",
    "        \n",
    "    def compile_network(self):\n",
    "        \n",
    "        optimizer = RMSprop(lr = 0.001)\n",
    "        self.model1.compile(optimizer = optimizer, \n",
    "                            loss = 'categorical_crossentropy', \n",
    "                            metrics = ['accuracy'])\n",
    "        \n",
    "    def fit_network(self, epochs = 10, batch_size = 32):\n",
    "        \n",
    "        self.model1.fit(x = self.X, \n",
    "                        y = self.y, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        shuffle = True)    \n",
    "        \n",
    "    \n",
    "    ### 100에폭 학습시켜놓은 모델 불러오기\n",
    "        \n",
    "    def load_network(self, model_file_path = 'saved_models\\\\text_model.h5'):\n",
    "        print('load model...')\n",
    "        self.model1 = load_model(model_file_path)\n",
    "        print('model loaded : {0}'.format(model_file_path))\n",
    "\n",
    "    def _sample_with_temp(self, preds, temperature = 1.0):\n",
    "        # 확률 배열에서 인덱스 하나를 샘플링하는 함수\n",
    "        # preds에는 원핫 카테고리별 확률값이 들어있는, 확률분포 리스트가 들어와야해.\n",
    "        \n",
    "        preds = np.array(preds).astype('float64')\n",
    "        exp_preds = np.exp( np.log(preds) / temperature ) \n",
    "        preds = exp_preds / np.sum(exp_preds)  # 이게 확률분포\n",
    "        probability = np.random.multinomial(1,preds,1) # 입력변수 각각 (주사위 몇번 던질래, 주사위 눈은몇개고 확률은 각각 어떻게 돼?, 이 결과값이 몇개 필요해?)\n",
    "        \n",
    "        return np.argmax(probability)\n",
    "    \n",
    "    def generate_text(self, seed_text, next_words, max_sequence_len=20 , temperature = 1):\n",
    "        \n",
    "        output_text = seed_text + ' '\n",
    "        seed_text = self.start_story + seed_text\n",
    "        \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "            token_list = token_list[-max_sequence_len:]\n",
    "            token_list = np.reshape(token_list, (1, max_sequence_len))  # model1.predict 에 넣어주려면 배치 형태로 넣어줘야 하니까 앞에 차원 하나 만들어줌\n",
    "            #print(type(token_list)) # numpy array\n",
    "            \n",
    "            probs = self.model1.predict(token_list, verbose = 0)[0]  # 배치형태로 값이 들어갔으니까, 출력도 배치형태라서 [0] 으로 차원 하나 풀어준다.\n",
    "            y_index = self._sample_with_temp(probs, temperature = temperature)\n",
    "            \n",
    "            output_word = self.tokenizer.index_word[y_index] if y_index > 0 else ''\n",
    "            # 신경망을 통해 뽑은 숫자를 글자로 다시 바꿔준다\n",
    "            # if절에서 y_index가 0보다 커야하는 조건 넣어준 이유는, tokenizer.index_word 딕셔너리는 key값 0이 없기 때문.\n",
    "            \n",
    "            if output_word == '|':\n",
    "                break\n",
    "            \n",
    "            seed_text += output_word + ' '\n",
    "            output_text += output_word + ' '\n",
    "            \n",
    "        return output_text\n",
    "    \n",
    "#### 단어 등장 확률 보고 싶어서 만들었다\n",
    "    \n",
    "    def _show_preds(self,preds):     \n",
    "            \n",
    "        preds = enumerate(preds)\n",
    "        preds = sorted(preds, key = lambda x:x[1], reverse = True )[:10]\n",
    "        \n",
    "        for i in range(10):\n",
    "            print('{0} : {1:10.2%} '.format(self.tokenizer.index_word[ preds[i][0] ], preds[i][1]))\n",
    "            \n",
    "    \n",
    "    def generate_text2(self, seed_text, next_words=1, max_sequence_len=20 , temperature = 1):\n",
    "        \n",
    "        output_text = seed_text + ' '\n",
    "        seed_text = self.start_story + seed_text\n",
    "        \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "            token_list = token_list[-max_sequence_len:]\n",
    "            token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "            #print(type(token_list)) # numpy array\n",
    "            \n",
    "            probs = self.model1.predict(token_list, verbose = 0)[0]\n",
    "            self._show_preds(probs)\n",
    "            \n",
    "\n",
    "t1 = text_LSTM()\n",
    "t1.text_loader(text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv')\n",
    "t1.tokenize()\n",
    "t1.generate_sequences()\n",
    "#t1.load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0\n",
    "for i,j in t1.tokenizer.index_word.items():\n",
    "    \n",
    "    print(i,':',j)\n",
    "    tmp += 1\n",
    "    if tmp > 100 : break\n",
    "\n",
    "t1.build_network()\n",
    "t1.compile_network()\n",
    "t1.fit_network(epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.generate_text('All this dissing', 100, temperature = 0.1))\n",
    "t1.generate_text2('All this dissing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = t1.tokenizer.texts_to_sequences( [t1.text])[0]\n",
    "print(aa.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_loader1(text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv' ):\n",
    "        \n",
    "    with open(text_file, encoding='utf-8-sig') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    \n",
    "    start_story = '| '  * 20\n",
    "    \n",
    "    # 텍스트 정제    \n",
    "    text = text.lower()\n",
    "    text = start_story + text\n",
    "    text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "    \n",
    "    text = re.sub('[\\n]{2,}','\\n\\n', text) \n",
    "    \n",
    "    #text = text.replace('\\n' , '\\n ') \n",
    "    #text = re.sub('\\n+[^\\s]' , r'\\1 \\n ',text) # 랩에 맞춰서 새로 만듬\n",
    "    \n",
    "    #text = re.sub('  +', '. ', text).strip()\n",
    "    #text = text.replace('..', '.')\n",
    "        \n",
    "    text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text) # 두번째 입력변수에 r' \\1 ' 대신 ' \\\\1 ' 써줘도 똑같음 - r 써주는 방법이 더 안전할 수 있대\n",
    "    #text = re.sub('\\s{5,}', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = text_loader1()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = 'hello, this is'\n",
    "\n",
    "output_text = seed_text\n",
    "seed_text = '| '*20 + seed_text\n",
    "print(seed_text)\n",
    "token_list = t1.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "print(token_list)\n",
    "token_list = token_list[-20:]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = np.reshape(token_list, (1, 20))\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 해보는 코드들\n",
    "mark = 613\n",
    "\n",
    "a = t1.model2.predict(t1.X[mark:mark+1])\n",
    "\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "\n",
    "# print(X[10])\n",
    "for i in X[mark]:\n",
    "    print(tokenizer.index_word[i])\n",
    "#print(tokenizer.index_word[2])\n",
    "print(tokenizer.index_word[ np.argmax(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model2.evaluate(X[mark:mark+1],a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.sequences_to_texts([[1,2,3,4,5]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multinomial(2, [1/6.]*6, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y[:3]:\n",
    "    print(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token_list[:50])\n",
    "aa = tokenizer.index_word\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,2,7,4,5]\n",
    "y2 = np_utils.to_categorical(y,8)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d9eead0949c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lyric.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'data.csv'\n",
    "\n",
    "with io.open(path, encoding = 'utf-8') as f :\n",
    "    corpus = f.read().lower()\n",
    "    \n",
    "corpus = re.sub('\\n', ' ', corpus)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_length = len(corpus)\n",
    "corpus_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(corpus)))\n",
    "total_words = len(words)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '3': 1, '4': 2, 'a': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'w': 21, 'x': 22, 'y': 23, '가': 24, '간': 25, '강': 26, '같': 27, '갚': 28, '개': 29, '거': 30, '건': 31, '것': 32, '게': 33, '겨': 34, '격': 35, '경': 36, '계': 37, '고': 38, '공': 39, '과': 40, '관': 41, '구': 42, '권': 43, '그': 44, '근': 45, '기': 46, '긴': 47, '까': 48, '깐': 49, '깽': 50, '꼭': 51, '꾼': 52, '끈': 53, '끝': 54, '나': 55, '난': 56, '날': 57, '남': 58, '내': 59, '냄': 60, '냐': 61, '너': 62, '넌': 63, '널': 64, '넘': 65, '네': 66, '녜': 67, '놈': 68, '누': 69, '느': 70, '는': 71, '니': 72, '다': 73, '닥': 74, '단': 75, '달': 76, '담': 77, '대': 78, '댈': 79, '더': 80, '덕': 81, '데': 82, '도': 83, '독': 84, '돈': 85, '동': 86, '돼': 87, '되': 88, '든': 89, '듣': 90, '들': 91, '등': 92, '딴': 93, '때': 94, '떤': 95, '똑': 96, '라': 97, '란': 98, '람': 99, '랑': 100, '래': 101, '랩': 102, '럭': 103, '럼': 104, '럽': 105, '레': 106, '려': 107, '력': 108, '로': 109, '론': 110, '른': 111, '를': 112, '리': 113, '린': 114, '마': 115, '만': 116, '많': 117, '말': 118, '망': 119, '맞': 120, '매': 121, '맨': 122, '맴': 123, '머': 124, '면': 125, '모': 126, '목': 127, '못': 128, '무': 129, '물': 130, '믿': 131, '바': 132, '박': 133, '반': 134, '발': 135, '밟': 136, '밥': 137, '뱀': 138, '뱅': 139, '번': 140, '벌': 141, '변': 142, '보': 143, '복': 144, '봐': 145, '부': 146, '불': 147, '비': 148, '빨': 149, '빵': 150, '사': 151, '산': 152, '상': 153, '새': 154, '색': 155, '생': 156, '서': 157, '선': 158, '성': 159, '세': 160, '소': 161, '솔': 162, '술': 163, '숨': 164, '쉬': 165, '스': 166, '슬': 167, '승': 168, '시': 169, '식': 170, '신': 171, '실': 172, '싸': 173, '아': 174, '악': 175, '알': 176, '앞': 177, '애': 178, '야': 179, '어': 180, '언': 181, '업': 182, '없': 183, '에': 184, '여': 185, '연': 186, '영': 187, '예': 188, '오': 189, '와': 190, '완': 191, '왕': 192, '왜': 193, '외': 194, '요': 195, '운': 196, '울': 197, '웃': 198, '원': 199, '위': 200, '윤': 201, '으': 202, '은': 203, '음': 204, '의': 205, '이': 206, '익': 207, '인': 208, '일': 209, '임': 210, '입': 211, '자': 212, '작': 213, '잔': 214, '잠': 215, '잣': 216, '장': 217, '잽': 218, '쟁': 219, '저': 220, '적': 221, '전': 222, '절': 223, '정': 224, '져': 225, '졌': 226, '족': 227, '졸': 228, '좀': 229, '좆': 230, '죽': 231, '즘': 232, '지': 233, '직': 234, '진': 235, '짐': 236, '집': 237, '차': 238, '착': 239, '찬': 240, '찰': 241, '참': 242, '처': 243, '척': 244, '체': 245, '취': 246, '치': 247, '컬': 248, '큰': 249, '탈': 250, '탓': 251, '탕': 252, '태': 253, '탱': 254, '투': 255, '파': 256, '퍼': 257, '하': 258, '한': 259, '할': 260, '함': 261, '항': 262, '해': 263, '행': 264, '허': 265, '형': 266, '혹': 267, '혼': 268, '회': 269, '후': 270, '희': 271, '히': 272}\n",
      "{0: ' ', 1: '3', 2: '4', 3: 'a', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'w', 22: 'x', 23: 'y', 24: '가', 25: '간', 26: '강', 27: '같', 28: '갚', 29: '개', 30: '거', 31: '건', 32: '것', 33: '게', 34: '겨', 35: '격', 36: '경', 37: '계', 38: '고', 39: '공', 40: '과', 41: '관', 42: '구', 43: '권', 44: '그', 45: '근', 46: '기', 47: '긴', 48: '까', 49: '깐', 50: '깽', 51: '꼭', 52: '꾼', 53: '끈', 54: '끝', 55: '나', 56: '난', 57: '날', 58: '남', 59: '내', 60: '냄', 61: '냐', 62: '너', 63: '넌', 64: '널', 65: '넘', 66: '네', 67: '녜', 68: '놈', 69: '누', 70: '느', 71: '는', 72: '니', 73: '다', 74: '닥', 75: '단', 76: '달', 77: '담', 78: '대', 79: '댈', 80: '더', 81: '덕', 82: '데', 83: '도', 84: '독', 85: '돈', 86: '동', 87: '돼', 88: '되', 89: '든', 90: '듣', 91: '들', 92: '등', 93: '딴', 94: '때', 95: '떤', 96: '똑', 97: '라', 98: '란', 99: '람', 100: '랑', 101: '래', 102: '랩', 103: '럭', 104: '럼', 105: '럽', 106: '레', 107: '려', 108: '력', 109: '로', 110: '론', 111: '른', 112: '를', 113: '리', 114: '린', 115: '마', 116: '만', 117: '많', 118: '말', 119: '망', 120: '맞', 121: '매', 122: '맨', 123: '맴', 124: '머', 125: '면', 126: '모', 127: '목', 128: '못', 129: '무', 130: '물', 131: '믿', 132: '바', 133: '박', 134: '반', 135: '발', 136: '밟', 137: '밥', 138: '뱀', 139: '뱅', 140: '번', 141: '벌', 142: '변', 143: '보', 144: '복', 145: '봐', 146: '부', 147: '불', 148: '비', 149: '빨', 150: '빵', 151: '사', 152: '산', 153: '상', 154: '새', 155: '색', 156: '생', 157: '서', 158: '선', 159: '성', 160: '세', 161: '소', 162: '솔', 163: '술', 164: '숨', 165: '쉬', 166: '스', 167: '슬', 168: '승', 169: '시', 170: '식', 171: '신', 172: '실', 173: '싸', 174: '아', 175: '악', 176: '알', 177: '앞', 178: '애', 179: '야', 180: '어', 181: '언', 182: '업', 183: '없', 184: '에', 185: '여', 186: '연', 187: '영', 188: '예', 189: '오', 190: '와', 191: '완', 192: '왕', 193: '왜', 194: '외', 195: '요', 196: '운', 197: '울', 198: '웃', 199: '원', 200: '위', 201: '윤', 202: '으', 203: '은', 204: '음', 205: '의', 206: '이', 207: '익', 208: '인', 209: '일', 210: '임', 211: '입', 212: '자', 213: '작', 214: '잔', 215: '잠', 216: '잣', 217: '장', 218: '잽', 219: '쟁', 220: '저', 221: '적', 222: '전', 223: '절', 224: '정', 225: '져', 226: '졌', 227: '족', 228: '졸', 229: '좀', 230: '좆', 231: '죽', 232: '즘', 233: '지', 234: '직', 235: '진', 236: '짐', 237: '집', 238: '차', 239: '착', 240: '찬', 241: '찰', 242: '참', 243: '처', 244: '척', 245: '체', 246: '취', 247: '치', 248: '컬', 249: '큰', 250: '탈', 251: '탓', 252: '탕', 253: '태', 254: '탱', 255: '투', 256: '파', 257: '퍼', 258: '하', 259: '한', 260: '할', 261: '함', 262: '항', 263: '해', 264: '행', 265: '허', 266: '형', 267: '혹', 268: '혼', 269: '회', 270: '후', 271: '희', 272: '히'}\n"
     ]
    }
   ],
   "source": [
    "words_indices = dict((c,i) for i, c in enumerate(words))\n",
    "indices_words = dict((i,c) for i, c in enumerate(words))\n",
    "\n",
    "print(words_indices)\n",
    "print(indices_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences :  339\n"
     ]
    }
   ],
   "source": [
    "max_length = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(corpus) - max_length, step) :\n",
    "    sentences.append(corpus[i:i+max_length])\n",
    "    next_chars.append(corpus[i+max_length])\n",
    "\n",
    "print('nb sequences : ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), max_length, len(words)), dtype = np.bool)\n",
    "y = np.zeros((len(sentences), len(words)), dtype = np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences) :\n",
    "    for j, word in enumerate(sentence) :\n",
    "        x[i, j, words_indices[word]] = 1\n",
    "    y[i, words_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape = (max_length, len(words))))\n",
    "model.add(Dense(len(words), activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop(lr = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature = 1.0) :\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.ramdon.multinomila(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _) :\n",
    "    start_index = random.randint(0, len(chrpus) - max_length -1)\n",
    "    generated = ''\n",
    "    sentence = corpus[start_index : start_index + max_length]\n",
    "    generated += sentence\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400) :\n",
    "        x_pred[0, t, word_indices[word]] = 1\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_word = indices_word[next_index]\n",
    "        \n",
    "        generated += next_word\n",
    "        sentence = sentence[1:] + next_word\n",
    "        \n",
    "        sys.stdout.write(next_word)\n",
    "        sys,stdout.flush()\n",
    "    print()\n",
    "    \n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, batch_size = 128, epochs = 60, callbacks = [print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
