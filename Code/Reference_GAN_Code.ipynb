{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project')\n",
    "os.chdir('C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bonnie\\Desktop\\Jupyter\\project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 원래 책에 있던 text_loader\n",
    "# =============================================================================\n",
    "# def text_loader(self, text_file = 'data\\\\aesop\\\\data.txt' ):\n",
    "#         \n",
    "#     with open(text_file, encoding='utf-8-sig') as f:\n",
    "#         text = f.read()\n",
    "#         \n",
    "#     \n",
    "#     self.start_story = '| '  * self.seq_length\n",
    "#     \n",
    "#     # 텍스트 정제    \n",
    "#     text = text.lower()\n",
    "#     text = self.start_story + text\n",
    "#     text = text.replace('\\n\\n\\n\\n\\n', self.start_story)\n",
    "#     text = text.replace('\\n',' ') \n",
    "#     \n",
    "#     text = re.sub('  +', '. ', text).strip()\n",
    "#     text = text.replace('..', '.')\n",
    "#     \n",
    "#     text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text)\n",
    "#     text = re.sub('\\s{2,}', ' ', text)\n",
    "#     \n",
    "#     self.text = text  \n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-239b3281f0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;31m#t1.load_network()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-239b3281f0ab>\u001b[0m in \u001b[0;36mgenerate_sequences\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# 1차원짜리 y값을 원핫 인코딩으로 변환 (total_words 개수만큼)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# to_categorical 얘는 그냥 0이면 1번쨰열을 1 나머지는 0, 55면 56번째 열을 1 나머지는 0 이런식으로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bonnie\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class text_LSTM:\n",
    "    def __init__(self):\n",
    "        self.seq_length = 20\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def text_loader(self, text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv' ):\n",
    "            \n",
    "        with open(text_file, encoding='utf-8-sig') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        self.start_story = '| '  * self.seq_length\n",
    "        \n",
    "        # 텍스트 정제    \n",
    "        text = text.lower()\n",
    "        text = self.start_story + text\n",
    "        text = text.replace('\\n\\n\\n\\n\\n', self.start_story) # 줄바꿈 5번 된건 끝나고 새로 시작하는걸로 처리\n",
    "        \n",
    "        text = re.sub('[\\n]{2,}','\\n', text) #\n",
    "        \n",
    "        text = text.replace('\\n','\\n ') # 랩에 맞춰서 새로 만듬\n",
    "        \n",
    "        #text = re.sub('  +', '. ', text).strip()\n",
    "        #text = text.replace('..', '.')\n",
    "        \n",
    "        text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text) # 두번째 입력변수에 r' \\1 ' 대신 ' \\\\1 ' 써줘도 똑같음 - r 써주는 방법이 더 안전할 수 있대\n",
    "        #text = re.sub('\\s{2,}', ' ', text)\n",
    "        \n",
    "        self.text = text\n",
    "    \n",
    "    def tokenize(self):\n",
    "        # 토큰화\n",
    "        self.tokenizer = Tokenizer(char_level = False, filters = '')         # 필터에 적혀있는 애는 토큰으로 안만든대\n",
    "        self.tokenizer.fit_on_texts([self.text])                             # 여기서 리스트 하나 더 안씌우면 문자단위로 토큰화 한다.\n",
    "        self.total_words = len(self.tokenizer.word_index) + 1                # 모든 단어종류 +1 // 1 더하는 이유는, 토크나이저.word_index가 1부터 인덱싱 해서, 뒤에 to_categorical로 원핫 인코딩할때 맞춰주기 위해서임\n",
    "        self.token_list = self.tokenizer.texts_to_sequences([self.text])[0]  # text의 모든 단어들을 토큰으로 바꾼거    \n",
    "    \n",
    "    ### seq_length만큼 잘라서, 신경망에 학습시킬 수 있도록 각 seq - 다음단어 쌍을 만들어준다.\n",
    "    \n",
    "    def generate_sequences(self, step = 1):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(0, len(self.token_list) - self.seq_length, step):\n",
    "            X.append(self.token_list[i:i+self.seq_length])\n",
    "            y.append(self.token_list[i + self.seq_length])\n",
    "            \n",
    "        y = np_utils.to_categorical(y, num_classes = self.total_words) \n",
    "        # 1차원짜리 y값을 원핫 인코딩으로 변환 (total_words 개수만큼)\n",
    "        # to_categorical 얘는 그냥 0이면 1번쨰열을 1 나머지는 0, 55면 56번째 열을 1 나머지는 0 이런식으로\n",
    "        # 숫자 값 그대로 원핫 인코딩 해주는 애\n",
    "        \n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.num_seq = len(X)        \n",
    "        \n",
    "        print('시퀀스 개수:', self.num_seq, '\\n')\n",
    "    \n",
    "    def build_network(self):\n",
    "        \n",
    "        n_units = 256\n",
    "        embedding_size = 100\n",
    "        \n",
    "        # 여기서부터 신경망 시작\n",
    "        \n",
    "        text_in = Input(shape = (None,))\n",
    "        x = Embedding(self.total_words, embedding_size)(text_in)\n",
    "        x = LSTM(units = n_units)(x)    # output의 차원은 n_units 개\n",
    "        x = Dropout(rate = 0.2)(x)\n",
    "        text_out = Dense(units = self.total_words, activation = 'softmax')(x)   # 아웃풋은 단어 종류의 개수만큼 표현이 되어야 하니까\n",
    "        \n",
    "        self.model1 = Model(text_in, text_out)        \n",
    "        \n",
    "    def compile_network(self):\n",
    "        \n",
    "        optimizer = RMSprop(lr = 0.001)\n",
    "        self.model1.compile(optimizer = optimizer, \n",
    "                            loss = 'categorical_crossentropy', \n",
    "                            metrics = ['accuracy'])\n",
    "        \n",
    "    def fit_network(self, epochs = 10, batch_size = 32):\n",
    "        \n",
    "        self.model1.fit(x = self.X, \n",
    "                        y = self.y, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size, \n",
    "                        shuffle = True)    \n",
    "        \n",
    "    \n",
    "    ### 100에폭 학습시켜놓은 모델 불러오기\n",
    "        \n",
    "    def load_network(self, model_file_path = 'saved_models\\\\text_model.h5'):\n",
    "        print('load model...')\n",
    "        self.model1 = load_model(model_file_path)\n",
    "        print('model loaded : {0}'.format(model_file_path))\n",
    "\n",
    "    def _sample_with_temp(self, preds, temperature = 1.0):\n",
    "        # 확률 배열에서 인덱스 하나를 샘플링하는 함수\n",
    "        # preds에는 원핫 카테고리별 확률값이 들어있는, 확률분포 리스트가 들어와야해.\n",
    "        \n",
    "        preds = np.array(preds).astype('float64')\n",
    "        exp_preds = np.exp( np.log(preds) / temperature ) \n",
    "        preds = exp_preds / np.sum(exp_preds)  # 이게 확률분포\n",
    "        probability = np.random.multinomial(1,preds,1) # 입력변수 각각 (주사위 몇번 던질래, 주사위 눈은몇개고 확률은 각각 어떻게 돼?, 이 결과값이 몇개 필요해?)\n",
    "        \n",
    "        return np.argmax(probability)\n",
    "    \n",
    "    def generate_text(self, seed_text, next_words, max_sequence_len=20 , temperature = 1):\n",
    "        \n",
    "        output_text = seed_text + ' '\n",
    "        seed_text = self.start_story + seed_text\n",
    "        \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "            token_list = token_list[-max_sequence_len:]\n",
    "            token_list = np.reshape(token_list, (1, max_sequence_len))  # model1.predict 에 넣어주려면 배치 형태로 넣어줘야 하니까 앞에 차원 하나 만들어줌\n",
    "            #print(type(token_list)) # numpy array\n",
    "            \n",
    "            probs = self.model1.predict(token_list, verbose = 0)[0]  # 배치형태로 값이 들어갔으니까, 출력도 배치형태라서 [0] 으로 차원 하나 풀어준다.\n",
    "            y_index = self._sample_with_temp(probs, temperature = temperature)\n",
    "            \n",
    "            output_word = self.tokenizer.index_word[y_index] if y_index > 0 else ''\n",
    "            # 신경망을 통해 뽑은 숫자를 글자로 다시 바꿔준다\n",
    "            # if절에서 y_index가 0보다 커야하는 조건 넣어준 이유는, tokenizer.index_word 딕셔너리는 key값 0이 없기 때문.\n",
    "            \n",
    "            if output_word == '|':\n",
    "                break\n",
    "            \n",
    "            seed_text += output_word + ' '\n",
    "            output_text += output_word + ' '\n",
    "            \n",
    "        return output_text\n",
    "    \n",
    "#### 단어 등장 확률 보고 싶어서 만들었다\n",
    "    \n",
    "    def _show_preds(self,preds):     \n",
    "            \n",
    "        preds = enumerate(preds)\n",
    "        preds = sorted(preds, key = lambda x:x[1], reverse = True )[:10]\n",
    "        \n",
    "        for i in range(10):\n",
    "            print('{0} : {1:10.2%} '.format(self.tokenizer.index_word[ preds[i][0] ], preds[i][1]))\n",
    "            \n",
    "    \n",
    "    def generate_text2(self, seed_text, next_words=1, max_sequence_len=20 , temperature = 1):\n",
    "        \n",
    "        output_text = seed_text + ' '\n",
    "        seed_text = self.start_story + seed_text\n",
    "        \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "            token_list = token_list[-max_sequence_len:]\n",
    "            token_list = np.reshape(token_list, (1, max_sequence_len))\n",
    "            #print(type(token_list)) # numpy array\n",
    "            \n",
    "            probs = self.model1.predict(token_list, verbose = 0)[0]\n",
    "            self._show_preds(probs)\n",
    "            \n",
    "\n",
    "t1 = text_LSTM()\n",
    "t1.text_loader(text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv')\n",
    "t1.tokenize()\n",
    "t1.generate_sequences()\n",
    "#t1.load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0\n",
    "for i,j in t1.tokenizer.index_word.items():\n",
    "    \n",
    "    print(i,':',j)\n",
    "    tmp += 1\n",
    "    if tmp > 100 : break\n",
    "\n",
    "t1.build_network()\n",
    "t1.compile_network()\n",
    "t1.fit_network(epochs = 5, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.generate_text('All this dissing', 100, temperature = 0.1))\n",
    "t1.generate_text2('All this dissing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = t1.tokenizer.texts_to_sequences( [t1.text])[0]\n",
    "print(aa.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_loader1(text_file = 'C:\\\\Users\\\\Bonnie\\\\Desktop\\\\Jupyter\\\\project\\\\data.csv' ):\n",
    "        \n",
    "    with open(text_file, encoding='utf-8-sig') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    \n",
    "    start_story = '| '  * 20\n",
    "    \n",
    "    # 텍스트 정제    \n",
    "    text = text.lower()\n",
    "    text = start_story + text\n",
    "    text = text.replace('\\n\\n\\n\\n\\n', start_story)\n",
    "    \n",
    "    text = re.sub('[\\n]{2,}','\\n\\n', text) \n",
    "    \n",
    "    #text = text.replace('\\n' , '\\n ') \n",
    "    #text = re.sub('\\n+[^\\s]' , r'\\1 \\n ',text) # 랩에 맞춰서 새로 만듬\n",
    "    \n",
    "    #text = re.sub('  +', '. ', text).strip()\n",
    "    #text = text.replace('..', '.')\n",
    "        \n",
    "    text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1 ', text) # 두번째 입력변수에 r' \\1 ' 대신 ' \\\\1 ' 써줘도 똑같음 - r 써주는 방법이 더 안전할 수 있대\n",
    "    #text = re.sub('\\s{5,}', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = text_loader1()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = 'hello, this is'\n",
    "\n",
    "output_text = seed_text\n",
    "seed_text = '| '*20 + seed_text\n",
    "print(seed_text)\n",
    "token_list = t1.tokenizer.texts_to_sequences([seed_text])[0]  # 글자들을 인덱스로 바꾼다. seed_text 겉에 리스트 씌워야 한다. 역으로 sequences_to_texts 할 때도 겉에 리스트 씌워야 한다.\n",
    "print(token_list)\n",
    "token_list = token_list[-20:]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = np.reshape(token_list, (1, 20))\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 해보는 코드들\n",
    "mark = 613\n",
    "\n",
    "a = t1.model2.predict(t1.X[mark:mark+1])\n",
    "\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "\n",
    "# print(X[10])\n",
    "for i in X[mark]:\n",
    "    print(tokenizer.index_word[i])\n",
    "#print(tokenizer.index_word[2])\n",
    "print(tokenizer.index_word[ np.argmax(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model2.evaluate(X[mark:mark+1],a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.sequences_to_texts([[1,2,3,4,5]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multinomial(2, [1/6.]*6, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y[:3]:\n",
    "    print(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token_list[:50])\n",
    "aa = tokenizer.index_word\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,2,7,4,5]\n",
    "y2 = np_utils.to_categorical(y,8)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
